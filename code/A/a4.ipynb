{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b869cbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Cell 1: Quick check\\nquick_validation_check()\\n\\n# Cell 2: Full validation  \\nresults = run_comprehensive_validation()\\n\\n# Cell 3: Access specific results\\nif results[\"validation_status\"] == \"PASSED\":\\n    print(\"‚úÖ Validation successful!\")\\n    # Access KPI results\\n    kpis = results[\"kpi_results\"]\\n    print(f\"Calculated {len(kpis)} KPIs\")\\nelse:\\n    print(\"‚ùå Validation failed:\", results.get(\"error\", \"Unknown error\"))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A.4 Data Validation and Analytics - Lab 3: Spark\n",
    "Location: A4_Data_Validation.ipynb\n",
    "\n",
    "VALIDATION OVERVIEW:\n",
    "1. Initialize Spark session with Delta Lake support\n",
    "2. Validate data integrity in Formatted Zone (3 datasets)\n",
    "3. Validate data integrity in Exploitation Zone (9+ datasets)\n",
    "4. Perform data quality checks and basic analytics\n",
    "5. Calculate and display key KPIs identified in A.1\n",
    "6. Validate cross-dataset relationships and joins\n",
    "7. Generate comprehensive validation report\n",
    "\n",
    "DATASETS VALIDATED:\n",
    "Formatted Zone:\n",
    "- idealista: Real estate property data\n",
    "- income: Socioeconomic income data  \n",
    "- cultural_sites: Cultural facility data\n",
    "\n",
    "Exploitation Zone:\n",
    "- property_analytics: Aggregated real estate metrics\n",
    "- socioeconomic_*_analytics: Income and demographic analytics\n",
    "- cultural_*_analytics: Cultural facility distribution\n",
    "- integrated_analytics: Combined dataset for composite KPIs\n",
    "- Additional analytical tables\n",
    "\n",
    "KPIs CALCULATED:\n",
    "- Housing market metrics (price/m¬≤, availability, distribution)\n",
    "- Socioeconomic indicators (inequality, affordability)\n",
    "- Cultural accessibility metrics (density, correlation)\n",
    "- Composite scores (attractiveness, equity)\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "\n",
    "# PySpark imports - importing functions separately to avoid conflicts\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    count,\n",
    "    sum as spark_sum,\n",
    "    avg,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev,\n",
    "    when,\n",
    "    isnan,\n",
    "    isnull,\n",
    "    desc,\n",
    "    asc,\n",
    "    round as spark_round,\n",
    "    percentile_approx,\n",
    "    collect_list,\n",
    "    size,\n",
    "    collect_set,\n",
    "    first,\n",
    "    last,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SPARK SESSION INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def initialize_spark_session() -> SparkSession:\n",
    "    \"\"\"Initialize Spark session with Delta Lake 4.0 support.\"\"\"\n",
    "    print(\"üöÄ Initializing Spark session with Delta Lake support...\")\n",
    "\n",
    "    try:\n",
    "        spark = (\n",
    "            SparkSession.builder.appName(\"BCN_DataValidation_A4\")\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.13:4.0.0\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\n",
    "                \"spark.sql.catalog.spark_catalog\",\n",
    "                \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "            )\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        print(\"‚úÖ Spark session initialized successfully\")\n",
    "        return spark\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize Spark: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA LOADING AND BASIC VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_and_validate_zones(\n",
    "    spark: SparkSession,\n",
    "    formatted_zone_path: str = \"formatted_zone\",\n",
    "    exploitation_zone_path: str = \"exploitation_zone\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Load and perform basic validation on both data zones.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìÇ LOADING AND VALIDATING DATA ZONES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = {\"formatted_zone\": {}, \"exploitation_zone\": {}, \"validation_summary\": {}}\n",
    "\n",
    "    # Formatted Zone datasets\n",
    "    formatted_datasets = [\"idealista\", \"income\", \"cultural_sites\"]\n",
    "\n",
    "    print(\"\\nüîç FORMATTED ZONE VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for dataset in formatted_datasets:\n",
    "        try:\n",
    "            path = f\"{formatted_zone_path}/{dataset}\"\n",
    "            df = spark.read.format(\"delta\").load(path)\n",
    "\n",
    "            record_count = df.count()\n",
    "            column_count = len(df.columns)\n",
    "\n",
    "            # Calculate null count using PySpark aggregation\n",
    "            null_counts = []\n",
    "            for column_name in df.columns:\n",
    "                null_count_for_col = df.filter(col(column_name).isNull()).count()\n",
    "                null_counts.append(null_count_for_col)\n",
    "\n",
    "            total_null_count = sum(null_counts)  # Python sum on Python list\n",
    "            total_cells = record_count * column_count\n",
    "            null_percentage = (\n",
    "                (total_null_count / total_cells * 100) if total_cells > 0 else 0\n",
    "            )\n",
    "\n",
    "            results[\"formatted_zone\"][dataset] = {\n",
    "                \"dataframe\": df,\n",
    "                \"record_count\": record_count,\n",
    "                \"column_count\": column_count,\n",
    "                \"null_count\": total_null_count,\n",
    "                \"null_percentage\": null_percentage,\n",
    "                \"path\": path,\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f\"‚úÖ {dataset.upper()}: {record_count:,} records, {column_count} columns, {null_percentage:.1f}% missing\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {dataset.upper()}: Failed to load - {str(e)}\")\n",
    "            results[\"formatted_zone\"][dataset] = {\"error\": str(e)}\n",
    "\n",
    "    # Exploitation Zone datasets\n",
    "    exploitation_datasets = [\n",
    "        \"property_analytics\",\n",
    "        \"property_type_analytics\",\n",
    "        \"socioeconomic_district_analytics\",\n",
    "        \"socioeconomic_neighborhood_analytics\",\n",
    "        \"income_quintiles\",\n",
    "        \"cultural_district_analytics\",\n",
    "        \"cultural_neighborhood_analytics\",\n",
    "        \"cultural_category_analytics\",\n",
    "        \"integrated_analytics\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüîç EXPLOITATION ZONE VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for dataset in exploitation_datasets:\n",
    "        try:\n",
    "            path = f\"{exploitation_zone_path}/{dataset}\"\n",
    "            df = spark.read.format(\"delta\").load(path)\n",
    "\n",
    "            record_count = df.count()\n",
    "            column_count = len(df.columns)\n",
    "\n",
    "            results[\"exploitation_zone\"][dataset] = {\n",
    "                \"dataframe\": df,\n",
    "                \"record_count\": record_count,\n",
    "                \"column_count\": column_count,\n",
    "                \"path\": path,\n",
    "            }\n",
    "\n",
    "            print(f\"‚úÖ {dataset}: {record_count:,} records, {column_count} columns\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {dataset}: Failed to load - {str(e)}\")\n",
    "            results[\"exploitation_zone\"][dataset] = {\"error\": str(e)}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATA QUALITY VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def perform_data_quality_checks(data_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Perform comprehensive data quality checks.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç DATA QUALITY VALIDATION CHECKS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    quality_results = {}\n",
    "\n",
    "    # Formatted Zone Quality Checks\n",
    "    print(\"\\nüìä FORMATTED ZONE QUALITY CHECKS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for dataset_name, dataset_info in data_results[\"formatted_zone\"].items():\n",
    "        if \"error\" in dataset_info:\n",
    "            continue\n",
    "\n",
    "        df = dataset_info[\"dataframe\"]\n",
    "        quality_checks = {}\n",
    "\n",
    "        print(f\"\\nüîé Analyzing {dataset_name.upper()}:\")\n",
    "\n",
    "        # 1. Schema validation\n",
    "        expected_columns = {\n",
    "            \"idealista\": [\n",
    "                \"property_code\",\n",
    "                \"district\",\n",
    "                \"neighborhood\",\n",
    "                \"price_eur\",\n",
    "                \"size_m2\",\n",
    "                \"price_per_m2\",\n",
    "            ],\n",
    "            \"income\": [\n",
    "                \"year\",\n",
    "                \"district_name\",\n",
    "                \"neighborhood_name\",\n",
    "                \"income_index_bcn_100\",\n",
    "            ],\n",
    "            \"cultural_sites\": [\"site_id\", \"facility_name\", \"district\", \"neighborhood\"],\n",
    "        }\n",
    "\n",
    "        if dataset_name in expected_columns:\n",
    "            missing_cols = set(expected_columns[dataset_name]) - set(df.columns)\n",
    "            quality_checks[\"missing_critical_columns\"] = list(missing_cols)\n",
    "            print(\n",
    "                f\"   ‚Ä¢ Critical columns: {'‚úÖ All present' if not missing_cols else f'‚ùå Missing: {missing_cols}'}\"\n",
    "            )\n",
    "\n",
    "        # 2. Duplicate detection using PySpark\n",
    "        total_count = df.count()\n",
    "        unique_count = df.distinct().count()\n",
    "        duplicate_count = total_count - unique_count\n",
    "        duplicate_percentage = (\n",
    "            (duplicate_count / total_count * 100) if total_count > 0 else 0\n",
    "        )\n",
    "\n",
    "        quality_checks[\"duplicate_count\"] = duplicate_count\n",
    "        quality_checks[\"duplicate_percentage\"] = duplicate_percentage\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Duplicates: {duplicate_count:,} records ({duplicate_percentage:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # 3. Dataset-specific validations using PySpark functions\n",
    "        if dataset_name == \"idealista\":\n",
    "            # Price validation using PySpark filter and count\n",
    "            negative_prices = df.filter(col(\"price_eur\") <= 0).count()\n",
    "            zero_sizes = df.filter(col(\"size_m2\") <= 0).count()\n",
    "            quality_checks[\"negative_prices\"] = negative_prices\n",
    "            quality_checks[\"zero_sizes\"] = zero_sizes\n",
    "            print(f\"   ‚Ä¢ Invalid prices: {negative_prices:,} records\")\n",
    "            print(f\"   ‚Ä¢ Invalid sizes: {zero_sizes:,} records\")\n",
    "\n",
    "        elif dataset_name == \"income\":\n",
    "            # Income validation using PySpark\n",
    "            negative_income = df.filter(col(\"income_index_bcn_100\") < 0).count()\n",
    "            future_years = df.filter(col(\"year\") > 2025).count()\n",
    "            quality_checks[\"negative_income\"] = negative_income\n",
    "            quality_checks[\"future_years\"] = future_years\n",
    "            print(f\"   ‚Ä¢ Negative income indices: {negative_income:,} records\")\n",
    "            print(f\"   ‚Ä¢ Future years: {future_years:,} records\")\n",
    "\n",
    "        quality_results[dataset_name] = quality_checks\n",
    "\n",
    "    # Exploitation Zone Quality Checks\n",
    "    print(\"\\nüìä EXPLOITATION ZONE QUALITY CHECKS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    key_datasets = [\"property_analytics\", \"integrated_analytics\"]\n",
    "\n",
    "    for dataset_name in key_datasets:\n",
    "        if (\n",
    "            dataset_name in data_results[\"exploitation_zone\"]\n",
    "            and \"error\" not in data_results[\"exploitation_zone\"][dataset_name]\n",
    "        ):\n",
    "            df = data_results[\"exploitation_zone\"][dataset_name][\"dataframe\"]\n",
    "\n",
    "            print(f\"\\nüîé Analyzing {dataset_name.upper()}:\")\n",
    "\n",
    "            # Check for aggregation consistency using PySpark\n",
    "            if dataset_name == \"property_analytics\":\n",
    "                # Verify no negative aggregated values\n",
    "                negative_prices = df.filter(col(\"avg_price_eur\") < 0).count()\n",
    "                negative_sizes = df.filter(col(\"avg_size_m2\") < 0).count()\n",
    "                print(f\"   ‚Ä¢ Negative average prices: {negative_prices:,} records\")\n",
    "                print(f\"   ‚Ä¢ Negative average sizes: {negative_sizes:,} records\")\n",
    "\n",
    "            elif dataset_name == \"integrated_analytics\":\n",
    "                # Check join completeness using PySpark\n",
    "                null_districts = df.filter(col(\"district\").isNull()).count()\n",
    "                null_prices = df.filter(col(\"median_price_eur\").isNull()).count()\n",
    "                null_income = df.filter(col(\"income_index_bcn_100\").isNull()).count()\n",
    "\n",
    "                print(f\"   ‚Ä¢ Missing districts: {null_districts:,} records\")\n",
    "                print(f\"   ‚Ä¢ Missing prices: {null_prices:,} records\")\n",
    "                print(f\"   ‚Ä¢ Missing income data: {null_income:,} records\")\n",
    "\n",
    "    return quality_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. KPI CALCULATIONS AND ANALYTICS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def calculate_housing_market_kpis(data_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate housing market KPIs.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üè† HOUSING MARKET KPI CALCULATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    kpi_results = {}\n",
    "\n",
    "    if \"property_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df = data_results[\"exploitation_zone\"][\"property_analytics\"][\"dataframe\"]\n",
    "\n",
    "        # KPI 1: Average Price per m¬≤ by District\n",
    "        print(\"\\nüìà KPI 1: Average Price per m¬≤ by District\")\n",
    "        district_prices = (\n",
    "            df.filter(col(\"analysis_level\") == \"district\")\n",
    "            .select(\"district\", \"avg_price_per_m2\", \"total_properties\")\n",
    "            .orderBy(col(\"avg_price_per_m2\").desc())\n",
    "        )\n",
    "\n",
    "        print(\"Top 5 Most Expensive Districts:\")\n",
    "        district_prices.show(5, truncate=False)\n",
    "\n",
    "        # Convert to pandas for storage and visualization\n",
    "        kpi_results[\"avg_price_per_m2_by_district\"] = district_prices.toPandas()\n",
    "\n",
    "        # KPI 2: Property Availability by District\n",
    "        print(\"\\nüìà KPI 2: Property Availability by District\")\n",
    "        availability = (\n",
    "            df.filter(col(\"analysis_level\") == \"district\")\n",
    "            .select(\"district\", \"total_properties\")\n",
    "            .orderBy(col(\"total_properties\").desc())\n",
    "        )\n",
    "\n",
    "        print(\"Districts by Property Availability:\")\n",
    "        availability.show(10, truncate=False)\n",
    "\n",
    "        kpi_results[\"property_availability\"] = availability.toPandas()\n",
    "\n",
    "    # Property Type Analysis\n",
    "    if \"property_type_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df_types = data_results[\"exploitation_zone\"][\"property_type_analytics\"][\n",
    "            \"dataframe\"\n",
    "        ]\n",
    "\n",
    "        print(\"\\nüìà KPI 3: Price Distribution by Property Type\")\n",
    "        type_analysis = (\n",
    "            df_types.groupBy(\"property_type\")\n",
    "            .agg(\n",
    "                avg(\"avg_price_per_m2_by_type\").alias(\"overall_avg_price_per_m2\"),\n",
    "                spark_sum(\"type_count\").alias(\"total_properties\"),\n",
    "            )\n",
    "            .orderBy(col(\"overall_avg_price_per_m2\").desc())\n",
    "        )\n",
    "\n",
    "        print(\"Property Types by Average Price per m¬≤:\")\n",
    "        type_analysis.show(truncate=False)\n",
    "\n",
    "        kpi_results[\"price_by_property_type\"] = type_analysis.toPandas()\n",
    "\n",
    "    return kpi_results\n",
    "\n",
    "\n",
    "def calculate_socioeconomic_kpis(data_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate socioeconomic KPIs.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí∞ SOCIOECONOMIC KPI CALCULATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    kpi_results = {}\n",
    "\n",
    "    if \"socioeconomic_district_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df = data_results[\"exploitation_zone\"][\"socioeconomic_district_analytics\"][\n",
    "            \"dataframe\"\n",
    "        ]\n",
    "\n",
    "        # KPI 4: Income Inequality Index\n",
    "        print(\"\\nüìà KPI 4: Income Inequality Index by District\")\n",
    "        inequality = df.select(\n",
    "            \"district_name\", \"income_inequality_cv\", \"avg_income_index\"\n",
    "        ).orderBy(col(\"income_inequality_cv\").desc())\n",
    "\n",
    "        print(\"Districts by Income Inequality (Coefficient of Variation):\")\n",
    "        inequality.show(10, truncate=False)\n",
    "\n",
    "        kpi_results[\"income_inequality\"] = inequality.toPandas()\n",
    "\n",
    "    # Income Quintile Distribution\n",
    "    if \"income_quintiles\" in data_results[\"exploitation_zone\"]:\n",
    "        df_quintiles = data_results[\"exploitation_zone\"][\"income_quintiles\"][\n",
    "            \"dataframe\"\n",
    "        ]\n",
    "\n",
    "        print(\"\\nüìà KPI 5: Income Quintile Distribution\")\n",
    "        quintile_dist = (\n",
    "            df_quintiles.groupBy(\"income_quintile\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"neighborhood_count\"),\n",
    "                avg(\"income_index_bcn_100\").alias(\"avg_income_index\"),\n",
    "            )\n",
    "            .orderBy(\"income_quintile\")\n",
    "        )\n",
    "\n",
    "        print(\"Income Distribution Across Quintiles:\")\n",
    "        quintile_dist.show(truncate=False)\n",
    "\n",
    "        kpi_results[\"income_quintiles\"] = quintile_dist.toPandas()\n",
    "\n",
    "    return kpi_results\n",
    "\n",
    "\n",
    "def calculate_cultural_accessibility_kpis(\n",
    "    data_results: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Calculate cultural accessibility KPIs.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üé≠ CULTURAL ACCESSIBILITY KPI CALCULATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    kpi_results = {}\n",
    "\n",
    "    if \"cultural_district_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df = data_results[\"exploitation_zone\"][\"cultural_district_analytics\"][\n",
    "            \"dataframe\"\n",
    "        ]\n",
    "\n",
    "        # KPI 7: Cultural Density by District\n",
    "        print(\"\\nüìà KPI 7: Cultural Density by District\")\n",
    "        cultural_density = df.select(\n",
    "            \"district\",\n",
    "            \"total_cultural_sites\",\n",
    "            \"cultural_sites_per_1000_residents\",\n",
    "            \"total_population\",\n",
    "        ).orderBy(col(\"cultural_sites_per_1000_residents\").desc())\n",
    "\n",
    "        print(\"Districts by Cultural Sites per 1000 Residents:\")\n",
    "        cultural_density.show(10, truncate=False)\n",
    "\n",
    "        kpi_results[\"cultural_density\"] = cultural_density.toPandas()\n",
    "\n",
    "    # Cultural Category Distribution\n",
    "    if \"cultural_category_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df_categories = data_results[\"exploitation_zone\"][\n",
    "            \"cultural_category_analytics\"\n",
    "        ][\"dataframe\"]\n",
    "\n",
    "        print(\"\\nüìà Cultural Facility Categories\")\n",
    "        category_dist = (\n",
    "            df_categories.groupBy(\"category\")\n",
    "            .agg(spark_sum(\"category_count\").alias(\"total_facilities\"))\n",
    "            .orderBy(col(\"total_facilities\").desc())\n",
    "        )\n",
    "\n",
    "        print(\"Cultural Facilities by Category:\")\n",
    "        category_dist.show(truncate=False)\n",
    "\n",
    "        kpi_results[\"cultural_categories\"] = category_dist.toPandas()\n",
    "\n",
    "    return kpi_results\n",
    "\n",
    "\n",
    "def calculate_composite_kpis(data_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate composite KPIs from integrated analytics.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä COMPOSITE KPI CALCULATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    kpi_results = {}\n",
    "\n",
    "    if \"integrated_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df = data_results[\"exploitation_zone\"][\"integrated_analytics\"][\"dataframe\"]\n",
    "\n",
    "        # KPI 8: Housing Affordability Analysis\n",
    "        print(\"\\nüìà KPI 8: Housing Affordability Ratio\")\n",
    "        affordability = (\n",
    "            df.filter(col(\"affordability_ratio\").isNotNull())\n",
    "            .select(\n",
    "                \"district\",\n",
    "                \"neighborhood\",\n",
    "                \"affordability_ratio\",\n",
    "                \"median_price_eur\",\n",
    "                \"income_index_bcn_100\",\n",
    "            )\n",
    "            .orderBy(col(\"affordability_ratio\").asc())\n",
    "        )\n",
    "\n",
    "        print(\"Most Affordable Neighborhoods (Lower ratio = more affordable):\")\n",
    "        affordability.show(10, truncate=False)\n",
    "\n",
    "        print(\"Least Affordable Neighborhoods:\")\n",
    "        affordability.orderBy(col(\"affordability_ratio\").desc()).show(\n",
    "            10, truncate=False\n",
    "        )\n",
    "\n",
    "        kpi_results[\"affordability_analysis\"] = affordability.toPandas()\n",
    "\n",
    "        # KPI 9: Neighborhood Attractiveness Score\n",
    "        print(\"\\nüìà KPI 9: Neighborhood Attractiveness Score\")\n",
    "        attractiveness = (\n",
    "            df.filter(col(\"attractiveness_score\").isNotNull())\n",
    "            .select(\n",
    "                \"district\",\n",
    "                \"neighborhood\",\n",
    "                \"attractiveness_score\",\n",
    "                \"income_index_bcn_100\",\n",
    "                \"cultural_sites_per_1000_residents\",\n",
    "                \"avg_price_per_m2\",\n",
    "            )\n",
    "            .orderBy(col(\"attractiveness_score\").desc())\n",
    "        )\n",
    "\n",
    "        print(\"Most Attractive Neighborhoods:\")\n",
    "        attractiveness.show(10, truncate=False)\n",
    "\n",
    "        kpi_results[\"neighborhood_attractiveness\"] = attractiveness.toPandas()\n",
    "\n",
    "        # KPI 10: Market Accessibility Analysis\n",
    "        print(\"\\nüìà KPI 10: Market Accessibility\")\n",
    "        market_access = (\n",
    "            df.groupBy(\"market_accessibility\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"neighborhood_count\"),\n",
    "                avg(\"total_properties\").alias(\"avg_properties\"),\n",
    "                avg(\"median_price_eur\").alias(\"avg_median_price\"),\n",
    "            )\n",
    "            .orderBy(\"market_accessibility\")\n",
    "        )\n",
    "\n",
    "        print(\"Market Accessibility Distribution:\")\n",
    "        market_access.show(truncate=False)\n",
    "\n",
    "        kpi_results[\"market_accessibility\"] = market_access.toPandas()\n",
    "\n",
    "    return kpi_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CROSS-DATASET RELATIONSHIP VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def validate_cross_dataset_relationships(\n",
    "    data_results: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Validate relationships and joins between datasets.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîó CROSS-DATASET RELATIONSHIP VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    validation_results = {}\n",
    "\n",
    "    # Check district/neighborhood consistency across datasets\n",
    "    if (\n",
    "        \"idealista\" in data_results[\"formatted_zone\"]\n",
    "        and \"income\" in data_results[\"formatted_zone\"]\n",
    "    ):\n",
    "        idealista_df = data_results[\"formatted_zone\"][\"idealista\"][\"dataframe\"]\n",
    "        income_df = data_results[\"formatted_zone\"][\"income\"][\"dataframe\"]\n",
    "\n",
    "        print(\"\\nüîç DISTRICT CONSISTENCY CHECK:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Get unique districts using PySpark operations\n",
    "        idealista_districts_df = (\n",
    "            idealista_df.select(\"district\")\n",
    "            .filter(col(\"district\").isNotNull())\n",
    "            .distinct()\n",
    "        )\n",
    "        income_districts_df = (\n",
    "            income_df.select(\"district_name\")\n",
    "            .filter(col(\"district_name\").isNotNull())\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        # Convert to Python sets for comparison\n",
    "        idealista_districts = set(\n",
    "            [row.district for row in idealista_districts_df.collect()]\n",
    "        )\n",
    "        income_districts = set(\n",
    "            [row.district_name for row in income_districts_df.collect()]\n",
    "        )\n",
    "\n",
    "        common_districts = idealista_districts.intersection(income_districts)\n",
    "        idealista_only = idealista_districts - income_districts\n",
    "        income_only = income_districts - idealista_districts\n",
    "\n",
    "        print(f\"‚úÖ Common districts: {len(common_districts)}\")\n",
    "        print(\n",
    "            f\"‚ö†Ô∏è  Idealista only: {len(idealista_only)} - {list(idealista_only)[:5]}...\"\n",
    "        )\n",
    "        print(f\"‚ö†Ô∏è  Income only: {len(income_only)} - {list(income_only)[:5]}...\")\n",
    "\n",
    "        validation_results[\"district_consistency\"] = {\n",
    "            \"common_count\": len(common_districts),\n",
    "            \"idealista_only_count\": len(idealista_only),\n",
    "            \"income_only_count\": len(income_only),\n",
    "        }\n",
    "\n",
    "    # Validate integrated analytics completeness\n",
    "    if \"integrated_analytics\" in data_results[\"exploitation_zone\"]:\n",
    "        df = data_results[\"exploitation_zone\"][\"integrated_analytics\"][\"dataframe\"]\n",
    "\n",
    "        print(\"\\nüîç INTEGRATED ANALYTICS COMPLETENESS:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        total_records = df.count()\n",
    "        complete_records = df.filter(\n",
    "            col(\"district\").isNotNull()\n",
    "            & col(\"neighborhood\").isNotNull()\n",
    "            & col(\"median_price_eur\").isNotNull()\n",
    "            & col(\"income_index_bcn_100\").isNotNull()\n",
    "        ).count()\n",
    "\n",
    "        completeness_rate = (\n",
    "            (complete_records / total_records * 100) if total_records > 0 else 0\n",
    "        )\n",
    "\n",
    "        print(f\"Total integrated records: {total_records:,}\")\n",
    "        print(f\"Complete records: {complete_records:,}\")\n",
    "        print(f\"Completeness rate: {completeness_rate:.1f}%\")\n",
    "\n",
    "        validation_results[\"integration_completeness\"] = {\n",
    "            \"total_records\": total_records,\n",
    "            \"complete_records\": complete_records,\n",
    "            \"completeness_rate\": completeness_rate,\n",
    "        }\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. VISUALIZATION AND SUMMARY REPORTING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def create_summary_visualizations(kpi_results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Create summary visualizations for key KPIs.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä GENERATING SUMMARY VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set up the plotting style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(\n",
    "        \"Barcelona Urban Analytics - Key KPIs Summary\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # Plot 1: Average Price per m¬≤ by District\n",
    "    if \"avg_price_per_m2_by_district\" in kpi_results:\n",
    "        df = kpi_results[\"avg_price_per_m2_by_district\"]\n",
    "        if not df.empty:\n",
    "            top_districts = df.head(8)\n",
    "            axes[0, 0].barh(\n",
    "                top_districts[\"district\"], top_districts[\"avg_price_per_m2\"]\n",
    "            )\n",
    "            axes[0, 0].set_title(\"Average Price per m¬≤ by District\")\n",
    "            axes[0, 0].set_xlabel(\"Price per m¬≤ (‚Ç¨)\")\n",
    "            axes[0, 0].tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "    # Plot 2: Income Inequality\n",
    "    if \"income_inequality\" in kpi_results:\n",
    "        df = kpi_results[\"income_inequality\"]\n",
    "        if not df.empty:\n",
    "            top_inequality = df.head(8)\n",
    "            axes[0, 1].bar(\n",
    "                range(len(top_inequality)), top_inequality[\"income_inequality_cv\"]\n",
    "            )\n",
    "            axes[0, 1].set_title(\"Income Inequality by District (CV)\")\n",
    "            axes[0, 1].set_ylabel(\"Coefficient of Variation\")\n",
    "            axes[0, 1].set_xticks(range(len(top_inequality)))\n",
    "            axes[0, 1].set_xticklabels(\n",
    "                top_inequality[\"district_name\"], rotation=45, ha=\"right\", fontsize=8\n",
    "            )\n",
    "\n",
    "    # Plot 3: Cultural Density\n",
    "    if \"cultural_density\" in kpi_results:\n",
    "        df = kpi_results[\"cultural_density\"]\n",
    "        if not df.empty:\n",
    "            df_clean = df.dropna(subset=[\"cultural_sites_per_1000_residents\"])\n",
    "            if not df_clean.empty:\n",
    "                axes[1, 0].scatter(\n",
    "                    df_clean[\"total_population\"],\n",
    "                    df_clean[\"cultural_sites_per_1000_residents\"],\n",
    "                )\n",
    "                axes[1, 0].set_title(\"Cultural Density vs Population\")\n",
    "                axes[1, 0].set_xlabel(\"Total Population\")\n",
    "                axes[1, 0].set_ylabel(\"Cultural Sites per 1000 Residents\")\n",
    "\n",
    "    # Plot 4: Affordability Distribution\n",
    "    if \"affordability_analysis\" in kpi_results:\n",
    "        df = kpi_results[\"affordability_analysis\"]\n",
    "        if not df.empty:\n",
    "            df_clean = df.dropna(subset=[\"affordability_ratio\"])\n",
    "            if not df_clean.empty and len(df_clean) > 0:\n",
    "                affordability_values = df_clean[\"affordability_ratio\"]\n",
    "                # Remove extreme outliers for better visualization\n",
    "                q75, q25 = np.percentile(affordability_values, [75, 25])\n",
    "                iqr = q75 - q25\n",
    "                lower_bound = q25 - (1.5 * iqr)\n",
    "                upper_bound = q75 + (1.5 * iqr)\n",
    "                filtered_values = affordability_values[\n",
    "                    (affordability_values >= lower_bound)\n",
    "                    & (affordability_values <= upper_bound)\n",
    "                ]\n",
    "\n",
    "                axes[1, 1].hist(filtered_values, bins=20, alpha=0.7)\n",
    "                axes[1, 1].set_title(\"Housing Affordability Distribution\")\n",
    "                axes[1, 1].set_xlabel(\"Affordability Ratio\")\n",
    "                axes[1, 1].set_ylabel(\"Number of Neighborhoods\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualizations generated successfully\")\n",
    "\n",
    "\n",
    "def generate_validation_report(\n",
    "    data_results: Dict[str, Any],\n",
    "    quality_results: Dict[str, Any],\n",
    "    kpi_results: Dict[str, Any],\n",
    "    relationship_results: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"Generate comprehensive validation report.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìã COMPREHENSIVE VALIDATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Summary Statistics\n",
    "    print(\"\\nüìä DATASET SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    formatted_total = sum(\n",
    "        [\n",
    "            info.get(\"record_count\", 0)\n",
    "            for info in data_results[\"formatted_zone\"].values()\n",
    "            if \"record_count\" in info\n",
    "        ]\n",
    "    )\n",
    "    exploitation_total = sum(\n",
    "        [\n",
    "            info.get(\"record_count\", 0)\n",
    "            for info in data_results[\"exploitation_zone\"].values()\n",
    "            if \"record_count\" in info\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Formatted Zone: {len(data_results['formatted_zone'])} datasets, {formatted_total:,} total records\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Exploitation Zone: {len(data_results['exploitation_zone'])} datasets, {exploitation_total:,} total records\"\n",
    "    )\n",
    "\n",
    "    # Data Quality Assessment\n",
    "    print(\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    quality_score = 0\n",
    "    quality_factors = 0\n",
    "\n",
    "    for dataset, checks in quality_results.items():\n",
    "        if \"duplicate_percentage\" in checks:\n",
    "            dup_score = max(0, 100 - checks[\"duplicate_percentage\"])\n",
    "            quality_score += dup_score\n",
    "            quality_factors += 1\n",
    "            print(f\"{dataset}: {dup_score:.1f}% quality (duplicates)\")\n",
    "\n",
    "    if quality_factors > 0:\n",
    "        overall_quality = quality_score / quality_factors\n",
    "        print(f\"\\nOverall Data Quality Score: {overall_quality:.1f}/100\")\n",
    "\n",
    "    # KPI Summary\n",
    "    print(\"\\nüìà KPI VALIDATION SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    kpi_count = len(\n",
    "        [\n",
    "            k\n",
    "            for k in kpi_results.keys()\n",
    "            if isinstance(kpi_results[k], pd.DataFrame) and not kpi_results[k].empty\n",
    "        ]\n",
    "    )\n",
    "    print(f\"‚úÖ Successfully calculated {kpi_count} KPIs\")\n",
    "\n",
    "    if (\n",
    "        \"avg_price_per_m2_by_district\" in kpi_results\n",
    "        and not kpi_results[\"avg_price_per_m2_by_district\"].empty\n",
    "    ):\n",
    "        price_df = kpi_results[\"avg_price_per_m2_by_district\"]\n",
    "        max_price = price_df[\"avg_price_per_m2\"].max()\n",
    "        min_price = price_df[\"avg_price_per_m2\"].min()\n",
    "        print(f\"Price range: {min_price:.0f}‚Ç¨ - {max_price:.0f}‚Ç¨ per m¬≤\")\n",
    "\n",
    "    if (\n",
    "        \"income_inequality\" in kpi_results\n",
    "        and not kpi_results[\"income_inequality\"].empty\n",
    "    ):\n",
    "        ineq_df = kpi_results[\"income_inequality\"]\n",
    "        max_ineq = ineq_df[\"income_inequality_cv\"].max()\n",
    "        print(f\"Max income inequality: {max_ineq:.1f}% CV\")\n",
    "\n",
    "    # Integration Assessment\n",
    "    print(\"\\nüîó DATA INTEGRATION ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if \"integration_completeness\" in relationship_results:\n",
    "        completeness = relationship_results[\"integration_completeness\"][\n",
    "            \"completeness_rate\"\n",
    "        ]\n",
    "        print(f\"Integration completeness: {completeness:.1f}%\")\n",
    "\n",
    "        if completeness >= 90:\n",
    "            print(\"‚úÖ Excellent integration quality\")\n",
    "        elif completeness >= 75:\n",
    "            print(\"‚ö†Ô∏è  Good integration quality\")\n",
    "        else:\n",
    "            print(\"‚ùå Poor integration quality - review joins\")\n",
    "\n",
    "    # Final Assessment\n",
    "    print(\"\\nüéØ FINAL ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    issues = []\n",
    "\n",
    "    if formatted_total == 0:\n",
    "        issues.append(\"No data in formatted zone\")\n",
    "    if exploitation_total == 0:\n",
    "        issues.append(\"No data in exploitation zone\")\n",
    "    if quality_factors > 0 and overall_quality < 70:\n",
    "        issues.append(f\"Low data quality ({overall_quality:.1f}%)\")\n",
    "    if kpi_count < 5:\n",
    "        issues.append(f\"Few KPIs calculated ({kpi_count})\")\n",
    "\n",
    "    if not issues:\n",
    "        print(\"‚úÖ ALL VALIDATIONS PASSED\")\n",
    "        print(\"üéâ Data pipeline is ready for production analysis!\")\n",
    "        print(\"\\nüìã Recommended next steps:\")\n",
    "        print(\"   ‚Ä¢ Proceed with advanced analytics and modeling\")\n",
    "        print(\"   ‚Ä¢ Create production dashboards\")\n",
    "        print(\"   ‚Ä¢ Implement automated monitoring\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  ISSUES IDENTIFIED:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "        print(\"\\nüìã Recommended actions:\")\n",
    "        print(\"   ‚Ä¢ Review and fix identified issues\")\n",
    "        print(\"   ‚Ä¢ Re-run data pipelines if necessary\")\n",
    "        print(\"   ‚Ä¢ Validate data sources\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. MAIN EXECUTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def run_comprehensive_validation(\n",
    "    formatted_zone_path: str = \"formatted_zone\",\n",
    "    exploitation_zone_path: str = \"exploitation_zone\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run comprehensive validation of the data pipeline.\n",
    "\n",
    "    Args:\n",
    "        formatted_zone_path: Path to the formatted zone directory\n",
    "        exploitation_zone_path: Path to the exploitation zone directory\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing all validation results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üöÄ STARTING COMPREHENSIVE DATA VALIDATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Initialize Spark\n",
    "    spark = initialize_spark_session()\n",
    "\n",
    "    try:\n",
    "        # 1. Load and validate zones\n",
    "        data_results = load_and_validate_zones(\n",
    "            spark, formatted_zone_path, exploitation_zone_path\n",
    "        )\n",
    "\n",
    "        # 2. Perform data quality checks\n",
    "        quality_results = perform_data_quality_checks(data_results)\n",
    "\n",
    "        # 3. Calculate KPIs\n",
    "        print(\"\\nüîÑ Calculating KPIs...\")\n",
    "        housing_kpis = calculate_housing_market_kpis(data_results)\n",
    "        socioeconomic_kpis = calculate_socioeconomic_kpis(data_results)\n",
    "        cultural_kpis = calculate_cultural_accessibility_kpis(data_results)\n",
    "        composite_kpis = calculate_composite_kpis(data_results)\n",
    "\n",
    "        # Combine all KPI results\n",
    "        all_kpi_results = {\n",
    "            **housing_kpis,\n",
    "            **socioeconomic_kpis,\n",
    "            **cultural_kpis,\n",
    "            **composite_kpis,\n",
    "        }\n",
    "\n",
    "        # 4. Validate cross-dataset relationships\n",
    "        relationship_results = validate_cross_dataset_relationships(data_results)\n",
    "\n",
    "        # 5. Create visualizations\n",
    "        create_summary_visualizations(all_kpi_results)\n",
    "\n",
    "        # 6. Generate final report\n",
    "        generate_validation_report(\n",
    "            data_results, quality_results, all_kpi_results, relationship_results\n",
    "        )\n",
    "\n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            \"data_results\": data_results,\n",
    "            \"quality_results\": quality_results,\n",
    "            \"kpi_results\": all_kpi_results,\n",
    "            \"relationship_results\": relationship_results,\n",
    "            \"validation_status\": \"PASSED\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå VALIDATION FAILED: {str(e)}\")\n",
    "        return {\"validation_status\": \"FAILED\", \"error\": str(e)}\n",
    "\n",
    "    finally:\n",
    "        # Clean up Spark session\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            print(\"\\nüîö Spark session closed\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. QUICK EXECUTION CELLS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def quick_validation_check():\n",
    "    \"\"\"Quick validation check - run this first to test setup.\"\"\"\n",
    "    print(\"üîç QUICK VALIDATION CHECK\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    spark = initialize_spark_session()\n",
    "\n",
    "    try:\n",
    "        # Test if zones exist and are accessible\n",
    "        formatted_path = \"formatted_zone\"\n",
    "        exploitation_path = \"exploitation_zone\"\n",
    "\n",
    "        # Check if paths exist\n",
    "        from pathlib import Path\n",
    "\n",
    "        if not Path(formatted_path).exists():\n",
    "            print(f\"‚ùå Formatted zone not found at: {formatted_path}\")\n",
    "            return False\n",
    "\n",
    "        if not Path(exploitation_path).exists():\n",
    "            print(f\"‚ùå Exploitation zone not found at: {exploitation_path}\")\n",
    "            return False\n",
    "\n",
    "        # Try to load one dataset from each zone\n",
    "        try:\n",
    "            df_idealista = spark.read.format(\"delta\").load(\n",
    "                f\"{formatted_path}/idealista\"\n",
    "            )\n",
    "            count_idealista = df_idealista.count()\n",
    "            print(\n",
    "                f\"‚úÖ Formatted zone accessible: {count_idealista:,} Idealista records\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Cannot access formatted zone: {e}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            df_integrated = spark.read.format(\"delta\").load(\n",
    "                f\"{exploitation_path}/integrated_analytics\"\n",
    "            )\n",
    "            count_integrated = df_integrated.count()\n",
    "            print(\n",
    "                f\"‚úÖ Exploitation zone accessible: {count_integrated:,} integrated records\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Cannot access exploitation zone: {e}\")\n",
    "            return False\n",
    "\n",
    "        print(\"‚úÖ Quick validation passed - ready for full validation!\")\n",
    "        return True\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "# Example usage in Jupyter:\n",
    "\"\"\"\n",
    "# Cell 1: Quick check\n",
    "quick_validation_check()\n",
    "\n",
    "# Cell 2: Full validation  \n",
    "results = run_comprehensive_validation()\n",
    "\n",
    "# Cell 3: Access specific results\n",
    "if results[\"validation_status\"] == \"PASSED\":\n",
    "    print(\"‚úÖ Validation successful!\")\n",
    "    # Access KPI results\n",
    "    kpis = results[\"kpi_results\"]\n",
    "    print(f\"Calculated {len(kpis)} KPIs\")\n",
    "else:\n",
    "    print(\"‚ùå Validation failed:\", results.get(\"error\", \"Unknown error\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5701a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUICK VALIDATION CHECK\n",
      "========================================\n",
      "üöÄ Initializing Spark session with Delta Lake support...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/20 16:58:19 WARN Utils: Your hostname, Monovo, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/06/20 16:58:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/m9o8/documents/bse/trimester3/dm/bdm3/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/m9o8/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/m9o8/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ffd16a2d-dced-4b4b-8481-7922d1953501;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 214ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ffd16a2d-dced-4b4b-8481-7922d1953501\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/06/20 16:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session initialized successfully\n",
      "‚ùå Formatted zone not found at: formatted_zone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_validation_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20afa495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING COMPREHENSIVE DATA VALIDATION\n",
      "================================================================================\n",
      "üöÄ Initializing Spark session with Delta Lake support...\n",
      "‚úÖ Spark session initialized successfully\n",
      "\n",
      "============================================================\n",
      "üìÇ LOADING AND VALIDATING DATA ZONES\n",
      "============================================================\n",
      "\n",
      "üîç FORMATTED ZONE VALIDATION:\n",
      "----------------------------------------\n",
      "idealista\n",
      "‚ùå IDEALISTA: Failed to load - [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n",
      "income\n",
      "‚ùå INCOME: Failed to load - [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n",
      "cultural_sites\n",
      "‚ùå CULTURAL_SITES: Failed to load - [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n",
      "\n",
      "üîç EXPLOITATION ZONE VALIDATION:\n",
      "----------------------------------------\n",
      "‚úÖ property_analytics: 85 records, 14 columns\n",
      "‚úÖ property_type_analytics: 73 records, 5 columns\n",
      "‚úÖ socioeconomic_district_analytics: 10 records, 12 columns\n",
      "‚úÖ socioeconomic_neighborhood_analytics: 73 records, 8 columns\n",
      "‚úÖ income_quintiles: 73 records, 4 columns\n",
      "‚úÖ cultural_district_analytics: 10 records, 7 columns\n",
      "‚úÖ cultural_neighborhood_analytics: 68 records, 8 columns\n",
      "‚úÖ cultural_category_analytics: 28 records, 3 columns\n",
      "‚úÖ integrated_analytics: 14 records, 17 columns\n",
      "\n",
      "============================================================\n",
      "üîç DATA QUALITY VALIDATION CHECKS\n",
      "============================================================\n",
      "\n",
      "üìä FORMATTED ZONE QUALITY CHECKS:\n",
      "----------------------------------------\n",
      "\n",
      "üìä EXPLOITATION ZONE QUALITY CHECKS:\n",
      "----------------------------------------\n",
      "\n",
      "üîé Analyzing PROPERTY_ANALYTICS:\n",
      "   ‚Ä¢ Negative average prices: 0 records\n",
      "   ‚Ä¢ Negative average sizes: 0 records\n",
      "\n",
      "üîé Analyzing INTEGRATED_ANALYTICS:\n",
      "   ‚Ä¢ Missing districts: 0 records\n",
      "   ‚Ä¢ Missing prices: 0 records\n",
      "   ‚Ä¢ Missing income data: 0 records\n",
      "\n",
      "üîÑ Calculating KPIs...\n",
      "\n",
      "============================================================\n",
      "üè† HOUSING MARKET KPI CALCULATIONS\n",
      "============================================================\n",
      "\n",
      "üìà KPI 1: Average Price per m¬≤ by District\n",
      "Top 5 Most Expensive Districts:\n",
      "+-------------------+----------------+----------------+\n",
      "|district           |avg_price_per_m2|total_properties|\n",
      "+-------------------+----------------+----------------+\n",
      "|Eixample           |5723.85         |1590            |\n",
      "|Sarri√†-Sant Gervasi|5539.08         |1694            |\n",
      "|Les Corts          |5224.08         |971             |\n",
      "|Ciutat Vella       |4863.69         |603             |\n",
      "|Gr√†cia             |4237.33         |368             |\n",
      "+-------------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üìà KPI 2: Property Availability by District\n",
      "Districts by Property Availability:\n",
      "+-------------------------+----------------+\n",
      "|district                 |total_properties|\n",
      "+-------------------------+----------------+\n",
      "|Sants-Montju√Øc           |2365            |\n",
      "|Sarri√†-Sant Gervasi      |1694            |\n",
      "|Eixample                 |1590            |\n",
      "|Centre                   |1412            |\n",
      "|La Florida - Les Planes  |983             |\n",
      "|Les Corts                |971             |\n",
      "|Santa Eul√†lia            |928             |\n",
      "|Can Serra - Pubilla Cases|832             |\n",
      "|La Torrasa               |732             |\n",
      "|Ciutat Vella             |603             |\n",
      "+-------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üìà KPI 3: Price Distribution by Property Type\n",
      "Property Types by Average Price per m¬≤:\n",
      "+-------------+------------------------+----------------+\n",
      "|property_type|overall_avg_price_per_m2|total_properties|\n",
      "+-------------+------------------------+----------------+\n",
      "|countryHouse |8373.0                  |1               |\n",
      "|duplex       |3870.1990076160396      |331             |\n",
      "|penthouse    |3726.6088778890203      |761             |\n",
      "|flat         |3409.820323016358       |11411           |\n",
      "|chalet       |3348.099802406593       |665             |\n",
      "|studio       |3034.2221583816513      |127             |\n",
      "+-------------+------------------------+----------------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üí∞ SOCIOECONOMIC KPI CALCULATIONS\n",
      "============================================================\n",
      "\n",
      "üìà KPI 4: Income Inequality Index by District\n",
      "Districts by Income Inequality (Coefficient of Variation):\n",
      "+-------------------+--------------------+------------------+\n",
      "|district_name      |income_inequality_cv|avg_income_index  |\n",
      "+-------------------+--------------------+------------------+\n",
      "|Les Corts          |47.26               |161.0             |\n",
      "|Sant Mart√≠         |37.15               |96.7              |\n",
      "|Eixample           |23.74               |122.63333333333333|\n",
      "|Sants-Montju√Øc     |23.64               |79.05000000000001 |\n",
      "|Ciutat Vella       |18.4                |89.07499999999999 |\n",
      "|Sant Andreu        |16.6                |70.37142857142858 |\n",
      "|Sarri√†-Sant Gervasi|16.12               |179.13333333333333|\n",
      "|Nou Barris         |14.44               |51.44615384615384 |\n",
      "|Horta-Guinard√≥     |14.32               |82.52727272727272 |\n",
      "|Gr√†cia             |9.61                |103.9             |\n",
      "+-------------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "üìà KPI 5: Income Quintile Distribution\n",
      "Income Distribution Across Quintiles:\n",
      "+---------------+------------------+------------------+\n",
      "|income_quintile|neighborhood_count|avg_income_index  |\n",
      "+---------------+------------------+------------------+\n",
      "|Q1_Lowest      |22                |54.46363636363637 |\n",
      "|Q2_Low         |18                |79.58333333333333 |\n",
      "|Q3_Medium      |9                 |95.34444444444445 |\n",
      "|Q4_High        |13                |107.55384615384615|\n",
      "|Q5_Highest     |11                |177.36363636363632|\n",
      "+---------------+------------------+------------------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üé≠ CULTURAL ACCESSIBILITY KPI CALCULATIONS\n",
      "============================================================\n",
      "\n",
      "üìà KPI 7: Cultural Density by District\n",
      "Districts by Cultural Sites per 1000 Residents:\n",
      "+-------------------+--------------------+---------------------------------+----------------+\n",
      "|district           |total_cultural_sites|cultural_sites_per_1000_residents|total_population|\n",
      "+-------------------+--------------------+---------------------------------+----------------+\n",
      "|Ciutat Vella       |124                 |1.21                             |102250.0        |\n",
      "|Gr√†cia             |135                 |1.11                             |121566.0        |\n",
      "|Les Corts          |75                  |0.91                             |82201.0         |\n",
      "|Sarri√†-Sant Gervasi|119                 |0.79                             |149734.0        |\n",
      "|Sant Andreu        |98                  |0.66                             |147693.0        |\n",
      "|Sants-Montju√Øc     |91                  |0.5                              |182354.0        |\n",
      "|Eixample           |88                  |0.33                             |267184.0        |\n",
      "|Horta-Guinard√≥     |52                  |0.31                             |169187.0        |\n",
      "|Sant Mart√≠         |60                  |0.25                             |236163.0        |\n",
      "|Nou Barris         |29                  |0.17                             |166805.0        |\n",
      "+-------------------+--------------------+---------------------------------+----------------+\n",
      "\n",
      "\n",
      "üìà Cultural Facility Categories\n",
      "Cultural Facilities by Category:\n",
      "+--------------------+----------------+\n",
      "|category            |total_facilities|\n",
      "+--------------------+----------------+\n",
      "|Informaci√≥ d'inter√®s|473             |\n",
      "|Variables           |81              |\n",
      "|Tel√®fons            |36              |\n",
      "|Responsables        |1               |\n",
      "+--------------------+----------------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä COMPOSITE KPI CALCULATIONS\n",
      "============================================================\n",
      "\n",
      "üìà KPI 8: Housing Affordability Ratio\n",
      "Most Affordable Neighborhoods (Lower ratio = more affordable):\n",
      "+--------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "|district      |neighborhood                 |affordability_ratio|median_price_eur|income_index_bcn_100|\n",
      "+--------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "|Nou Barris    |Verdun                       |2.63               |135000.0        |51.3                |\n",
      "|Sants-Montju√Øc|Hostafrancs                  |2.63               |260000.0        |99.0                |\n",
      "|Sants-Montju√Øc|Sants - Badal                |2.81               |228000.0        |81.0                |\n",
      "|Nou Barris    |Porta                        |2.95               |190000.0        |64.4                |\n",
      "|Sants-Montju√Øc|Sants                        |3.03               |300000.0        |99.0                |\n",
      "|Gr√†cia        |Vallcarca i els Penitents    |3.73               |420000.0        |112.5               |\n",
      "|Sant Andreu   |Navas                        |3.79               |309000.0        |81.6                |\n",
      "|Nou Barris    |Canyelles                    |3.79               |198000.0        |52.2                |\n",
      "|Nou Barris    |Vilapicina i la Torre Llobeta|4.22               |269000.0        |63.8                |\n",
      "|Eixample      |Sant Antoni                  |4.32               |450000.0        |104.2               |\n",
      "+--------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "Least Affordable Neighborhoods:\n",
      "+-------------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "|district           |neighborhood                 |affordability_ratio|median_price_eur|income_index_bcn_100|\n",
      "+-------------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "|Sarri√†-Sant Gervasi|Sarri√†                       |5.94               |1150000.0       |193.6               |\n",
      "|Sarri√†-Sant Gervasi|Sant Gervasi - Galvany       |5.0                |960000.0        |192.1               |\n",
      "|Sant Andreu        |Sant Andreu                  |4.63               |359900.0        |77.7                |\n",
      "|Les Corts          |Pedralbes                    |4.62               |1150000.0       |248.8               |\n",
      "|Eixample           |Sant Antoni                  |4.32               |450000.0        |104.2               |\n",
      "|Nou Barris         |Vilapicina i la Torre Llobeta|4.22               |269000.0        |63.8                |\n",
      "|Sant Andreu        |Navas                        |3.79               |309000.0        |81.6                |\n",
      "|Nou Barris         |Canyelles                    |3.79               |198000.0        |52.2                |\n",
      "|Gr√†cia             |Vallcarca i els Penitents    |3.73               |420000.0        |112.5               |\n",
      "|Sants-Montju√Øc     |Sants                        |3.03               |300000.0        |99.0                |\n",
      "+-------------------+-----------------------------+-------------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üìà KPI 9: Neighborhood Attractiveness Score\n",
      "Most Attractive Neighborhoods:\n",
      "+-------------------+-------------------------+--------------------+--------------------+---------------------------------+----------------+\n",
      "|district           |neighborhood             |attractiveness_score|income_index_bcn_100|cultural_sites_per_1000_residents|avg_price_per_m2|\n",
      "+-------------------+-------------------------+--------------------+--------------------+---------------------------------+----------------+\n",
      "|Les Corts          |Pedralbes                |88.64               |248.8               |2.56                             |6331.71         |\n",
      "|Sarri√†-Sant Gervasi|Sarri√†                   |68.5                |193.6               |1.16                             |5760.56         |\n",
      "|Sarri√†-Sant Gervasi|Sant Gervasi - Galvany   |65.35               |192.1               |0.27                             |5785.72         |\n",
      "|Gr√†cia             |Vallcarca i els Penitents|45.3                |112.5               |0.9                              |4519.67         |\n",
      "|Sant Andreu        |Sant Andreu              |44.43               |77.7                |1.08                             |2237.5          |\n",
      "|Sants-Montju√Øc     |Hostafrancs              |41.74               |99.0                |0.38                             |3670.26         |\n",
      "|Sants-Montju√Øc     |Sants                    |41.63               |99.0                |0.32                             |3645.01         |\n",
      "|Eixample           |Sant Antoni              |40.44               |104.2               |0.16                             |4598.77         |\n",
      "|Sants-Montju√Øc     |Sants - Badal            |35.62               |81.0                |0.08                             |3609.58         |\n",
      "|Nou Barris         |Porta                    |33.64               |64.4                |0.2                              |2916.17         |\n",
      "+-------------------+-------------------------+--------------------+--------------------+---------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üìà KPI 10: Market Accessibility\n",
      "Market Accessibility Distribution:\n",
      "+--------------------+------------------+------------------+------------------+\n",
      "|market_accessibility|neighborhood_count|avg_properties    |avg_median_price  |\n",
      "+--------------------+------------------+------------------+------------------+\n",
      "|High                |7                 |291.14285714285717|642571.4285714285 |\n",
      "|Low                 |6                 |4.166666666666667 |243483.33333333334|\n",
      "|Medium              |1                 |79.0              |420000.0          |\n",
      "+--------------------+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîó CROSS-DATASET RELATIONSHIP VALIDATION\n",
      "============================================================\n",
      "\n",
      "‚ùå VALIDATION FAILED: 'dataframe'\n",
      "\n",
      "üîö Spark session closed\n"
     ]
    }
   ],
   "source": [
    "results = run_comprehensive_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
