<h1 align="center">ğŸš€ BDM3: Data Lake Architecture with Spark, MLflow & Airflow</h1>
<p align="center">
  <b>Julian Romero & Moritz Peist</b><br>
  Barcelona School of Economics Â· 2025
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python">
  <img src="https://img.shields.io/badge/PySpark-4.0-orange?logo=apachespark&logoColor=white">
  <img src="https://img.shields.io/badge/MLflow-3.0.0-blue?logo=mlflow">
  <img src="https://img.shields.io/badge/Airflow-3.0.2-darkgreen?logo=apacheairflow">
  <img src="https://img.shields.io/badge/Streamlit-App-red?logo=streamlit">
</p>

---

## ğŸ“‘ Table of Contents

- [ğŸ§  Project Overview](#-project-overview)
- [âœ¨ Features](#-features)
- [ğŸ› ï¸ Setup & Running](#ï¸-setup--running)
  - [ğŸ“¦ Prerequisites](#1--prerequisites)
  - [ğŸš€ Start Everything (Docker)](#2--start-everything-docker)
  - [ğŸ§ª Run Notebooks (Optional)](#3--run-notebooks-optional)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ§© Technologies Used](#-technologies-used)



## ğŸ§  Project Overview

This project implements a **modular data lake architecture** and pipeline ecosystem as part of the [Lab 3 assignment](./Lab3-Assignment-Statement.pdf) for the course **Big Data Management for Data Science**.

It simulates:
- A local **data lake** with structured zones: **Landing**, **Formatted**, and **Exploitation**.
- A **Spark-based ETL pipeline**.
- **ML model training and tracking** via MLflow.
- **Pipeline orchestration** with Airflow.
- A unified **Streamlit UI** to navigate and interact with everything.

The datasets selected for the project:

- Idealista data (house prices).
- Income data.
- Cultural data.



> âš ï¸ **Disclaimer I**: The Streamlit UI was designed specifically for the datasets used during this project. It may require significant adjustments in terms of **validation, scalability, and security** to support production-grade applications.

> âš ï¸ **Disclaimer II**: The ML modeling and MLflow tracking components are included to **demonstrate the integration of Big Data tools**, not to optimize model performance. The focus is on **infrastructure and pipeline design**.

> âš ï¸ **Disclaimer III**: The datasets used in this project originate from different sources and cover **non-overlapping time periods**. This inconsistency was **intentionally overlooked** to simulate a working end-to-end ML pipeline for predicting housing prices. The primary objective is to demonstrate the use of Big Data toolsâ€”not to achieve rigorous modeling accuracy.


## âœ¨ Features

- ğŸ”„ **Data ingestion pipeline** from raw JSON/CSV into clean Parquet format
- ğŸ§¼ **Data cleaning, enrichment**, and formatting with PySpark
- ğŸ“Š **Exploratory Data Analysis** and KPIs in Streamlit
- ğŸ§  **Model training** with MLflow tracking (accuracy, recall, parameters)
- ğŸª„ **Automatic deployment** of the best model
- âš™ï¸ **Airflow DAG orchestration**
- ğŸ“‚ Upload raw datasets via UI and browse zone contents dynamically



## ğŸ› ï¸ Setup & Running

### 1. ğŸ“¦ Prerequisites
- [Docker](https://www.docker.com/) and [Docker Compose](https://docs.docker.com/compose/)
- Optional: [uv](https://github.com/astral-sh/uv) for Python environment management

---

### 2. ğŸš€ Start Everything (Docker)
The following command should be run to build the docker image and start the services:

```bash
docker compose up
````

The services are pointing to the following ports of your local machine, where they all are accessible from the Streamlit App UI:

* Streamlit App: [http://localhost:8501](http://localhost:8501)
* Airflow UI: [http://localhost:8080](http://localhost:8080)
* MLflow UI: [http://localhost:5001](http://localhost:5001)

You can upload data, trigger processing, monitor pipelines, and track ML models â€” all from the app.

---

### 3. ğŸ§ª Run Notebooks (Optional)

Install [`uv`](https://github.com/astral-sh/uv) and sync the Python environment:

```bash
uv sync
```


## ğŸ“ Project Structure

```bash
bdm3/
â”œâ”€â”€ data_zones/                 # ğŸ—‚ï¸ Simulated data lake zones
â”‚   â”œâ”€â”€ 01_landing/             # Raw uploaded files
â”‚   â”œâ”€â”€ 02_formatted/           # Cleaned & standardized
â”‚   â”œâ”€â”€ 03_exploitation/        # Feature-ready data
â”‚
â”œâ”€â”€ notebooks/                  # ğŸ““ Exploratory notebooks
â”‚   â”œâ”€â”€ a1.ipynb
â”‚   â”œâ”€â”€ a2.ipynb
â”‚   â””â”€â”€ a3.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ mlruns/                 # ğŸ“ MLflow tracking data
â”‚
â”œâ”€â”€ src/                        # âš™ï¸ Application logic
â”‚   â”œâ”€â”€ airflow/                # DAGs & Airflow Dockerfile
â”‚   â”œâ”€â”€ ml/                     # Model training & selection
â”‚   â”œâ”€â”€ pipelines/              # Spark-based ETL jobs
â”‚   â”œâ”€â”€ streamlit_ui/           # Streamlit app (multi-page)
â”‚   â””â”€â”€ utils/                  # Shared logic/utilities
â”‚
â”œâ”€â”€ .env.example                # ğŸ”§ Environment config example
â”œâ”€â”€ docker-compose.yml          # ğŸ³ Service orchestration
â”œâ”€â”€ pyproject.toml              # ğŸ“¦ Project dependencies (via uv/pip)
â”œâ”€â”€ uv.lock                     # ğŸ”’ Dependency lock file
â”œâ”€â”€ .python-version             # Python version (3.11)
â””â”€â”€ README.md                   # ğŸ“– This file
```

---

## ğŸ§© Technologies Used

* **Apache Spark 4.0 (PySpark)** â€“ Distributed data processing
* **MLflow 2.0.1** â€“ Model management and experiment tracking
* **Apache Airflow 3.0.2** â€“ Workflow scheduling and orchestration
* **Streamlit** â€“ UI for uploads, previews, and dashboards
* **Docker** â€“ Reproducible, containerized development
