
\section{Orchestration Framework (Bonus)}

\textbf{Implementation:} Apache Airflow 3.0 with asset-based scheduling

Our orchestration solution provides production-ready pipeline management:

\begin{itemize}[nosep]
\item \textbf{DAG Architecture:} 
\begin{itemize}
    \item \texttt{bcn\_data\_pipeline\_with\_validation} orchestrates A.2 → A.3 → A.4 pipeline sequence.
    \item \texttt{train\_and\_deploy\_model} orchestrates B, from training to deployment.
\end{itemize}

\item \textbf{Dependency Management:} Asset-based scheduling ensures proper execution order and data availability
\item \textbf{Error Handling:} Comprehensive failure recovery with email notifications and retry mechanisms
\item \textbf{Monitoring Integration:} Real-time pipeline status through Airflow UI at \texttt{localhost:8080}
\end{itemize}

\textbf{Advanced Features:} Docker containerization for reproducibility, configuration management through environment variables, and integration with MLflow for model deployment.

\textbf{Pipeline Flow:} Landing Zone (raw data) → Formatted Zone (Delta tables) → Exploitation Zone (analytics datasets) → Validation Reports → Streamlit Dashboards

\textbf{Technology Integration:} Our solution integrates PySpark 4.0 for distributed processing, Delta Lake for ACID transactions, Airflow 3.0 for orchestration, MlFlow for model monitoring/deployment, and Streamlit for interactive visualization.